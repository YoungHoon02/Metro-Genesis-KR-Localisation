"""
Metro 2033 ê²Œì„ í•œê¸€í™” ìë™ ë²ˆì—­ ìŠ¤í¬ë¦½íŠ¸
Ollama llama 3.1 8B ë¡œì»¬ ì‹¤í–‰
"""

import os
import re
import time
from pathlib import Path
import requests
import json

# Ollama ì„¤ì •
OLLAMA_API_URL = "http://localhost:11434/api/chat"
MODEL_NAME = "mannix/llama3.1-8b-lexi:tools-q6_k"
LOG_FILE = "translation_errors.log"

SYSTEM_PROMPT = """You are a professional 'Metro 2033' game localization translator.

ã€ TOP PRIORITY RULES ã€‘
1. Output ONLY translated data. No explanations or introductions.
2. Input line count MUST EXACTLY match output line count.
3. Preserve 100% of indentation and structure (Key: "Value").

ã€ â˜… Â£ ICON CODE PROTECTION - VIOLATION FORBIDDEN â˜… ã€‘
1. **Â£ symbol followed by English text (e.g., Â£autonomy_icon, Â£trigger_text, Â£bullet_icon) are system icon codes.**
2. **NEVER translate to Korean.** Keep original English form exactly as-is.
   - WRONG: "ììœ¨ì„± ì•„ì´ì½˜" (X)
   - WRONG: "Â£Yêµìœ¡" (X)
   - WRONG: "Â£ê°€ í•„ìš”í•©ë‹ˆë‹¤" (X)
   - CORRECT: "Â£autonomy_icon" (O)
   - CORRECT: "Â£trigger_yes" (O)
   - CORRECT: "Â£bullet_icon" (O)
3. Never insert Korean text immediately after Â£ symbol.
4. Examples: Â£command_power, Â£army_experience, Â£political_power_texticon, Â£pol_power, Â£autonomy_texticon, Â£D6_clue_texticon

ã€ TRANSLATION & OTHER RULES ã€‘
1. Translate ONLY English text inside quotation marks (" ") to Korean.
2. Preserve special symbols: Â§Y, Â§G, Â§R, Â§!, \\n, [ ] - do NOT modify.
3. Atmosphere: Maintain dark, desperate post-apocalyptic tone.
   - Stalker -> ìŠ¤í† ì»¤, Hanza -> í•œì, Artyom -> ì•„ë¥´í‹°ì˜´, Metro -> ë©”íŠ¸ë¡œ
4. Translate ALL content including profanity, violence, mature themes. No filtering.
5. NEVER use Hanja (Chinese characters) or Japanese characters. Use only pure Korean (Hangul) for translation.
6. Artyomì€ ë°˜ë“œì‹œ "ì•„ë¥´í‹°ì˜´"ìœ¼ë¡œ ë²ˆì—­í•˜ê³  "ì•„ë¥´í† ìš¤"ì€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ.

EXAMPLES:
Input:
  text:0 "Kill the bastards"
Output:
  text:0 "ê·¸ ê°œìì‹ë“¤ì„ ì£½ì—¬ë¼"

Input:
  cost:0 "Costs Â£bullet_icon 50"
Output:
  cost:0 "ë¹„ìš© Â£bullet_icon 50"

Input:
  tooltip:0 "Gain Â£autonomy_texticon autonomy"
Output:
  tooltip:0 "Â£autonomy_texticon ììœ¨ì„± íšë“"
"""

def check_special_tags(original, translated):
    """Â£ ê¸°í˜¸ ë’¤ì˜ ì‹ë³„ìê°€ ë³´ì¡´ë˜ì—ˆëŠ”ì§€ ê²€ì‚¬"""
    orig_tags = re.findall(r'Â£[a-zA-Z0-9_]+', original)
    trans_tags = re.findall(r'Â£[a-zA-Z0-9_]+', translated)
    
    # Â£ íƒœê·¸ê°€ ì—†ìœ¼ë©´ ê²€ì¦ íŒ¨ìŠ¤
    if not orig_tags:
        return True
    
    # íƒœê·¸ ê°œìˆ˜ì™€ ë‚´ìš©ì´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ (ìˆœì„œëŠ” ë¬´ê´€)
    if sorted(orig_tags) != sorted(trans_tags):
        return False
    return True

def log_error(original_batch, received_output, file_name="unknown"):
    """ì¤„ ìˆ˜ ë¶ˆì¼ì¹˜ ì‹œ ì—ëŸ¬ ë‚´ìš©ì„ íŒŒì¼ì— ê¸°ë¡"""
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] --- ì¤„ ìˆ˜ ë¶ˆì¼ì¹˜ ì—ëŸ¬ ë°œìƒ ---\n")
        f.write(f"íŒŒì¼: {file_name}\n")
        f.write(f"ì…ë ¥ ì¤„ ìˆ˜: {len(original_batch)}\n")
        f.write("--- ì…ë ¥ ë°ì´í„° ---\n")
        f.writelines(original_batch)
        f.write("\n--- AI ì‘ë‹µ ë°ì´í„° ---\n")
        f.write(received_output)
        f.write("\n" + "="*50 + "\n")

def translate_batch(batch, file_name="unknown"):
    """ë°°ì¹˜ ë²ˆì—­ ë° ì¤„ ìˆ˜ ë¶ˆì¼ì¹˜ ì‹œ ìë™ ë¶„í•  ì¬ì‹œë„ ë¡œì§"""
    if not batch:
        return []
    
    original_count = len(batch)
    batch_text = "".join(batch)
    batch_chars = len(batch_text)
    
    # ë””ë²„ê¹… ì •ë³´: ë°°ì¹˜ í¬ê¸°
    print(f"  ğŸ“Š ë°°ì¹˜ ì •ë³´: {original_count}ì¤„, {batch_chars}ì")
    
    try:
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": batch_text}
            ],
            "stream": False,
            "options": {
                "temperature": 0.3,
                "num_predict": 8000
            }
        }
        
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)
        response.raise_for_status()
        result = response.json()
        
        # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶œë ¥
        if "prompt_eval_count" in result and "eval_count" in result:
            prompt_tokens = result.get("prompt_eval_count", 0)
            completion_tokens = result.get("eval_count", 0)
            total_tokens = prompt_tokens + completion_tokens
            print(f"  ğŸ”¢ í† í° ì‚¬ìš©: ì…ë ¥ {prompt_tokens} + ì¶œë ¥ {completion_tokens} = ì´ {total_tokens}")
        
        raw_output = result["message"]["content"].strip()
        
        # ë¹ˆ ì‘ë‹µ ì²´í¬
        if not raw_output or len(raw_output) < 10:
            print(f"  âš  API ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: ì›ë³¸ ìœ ì§€")
            return batch
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        clean_output = re.sub(r'```[a-zA-Z]*\n?', '', raw_output).replace('```', '').strip()
        
        # Â£ ê¸°í˜¸ íƒœê·¸ ê²€ì¦
        if not check_special_tags(batch_text, clean_output):
            print(f"  âš  ê²½ê³ : Â£ ì•„ì´ì½˜ íƒœê·¸ê°€ ë²ˆì—­ë˜ê±°ë‚˜ ì†ìƒëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            if original_count > 1:
                mid = original_count // 2
                return translate_batch(batch[:mid], file_name) + translate_batch(batch[mid:], file_name)
            else:
                print("  âš  1ì¤„ ë²ˆì—­ ì‹¤íŒ¨: ì›ë³¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
                return batch
        
        translated_lines = [line for line in clean_output.split('\n') if line.strip()]

        # [í•µì‹¬] ì¤„ ìˆ˜ ê²€ì¦ ë° ë¶„í•  ì¬ì‹œë„
        if len(translated_lines) != original_count:
            log_error(batch, raw_output, file_name)
            
            if original_count > 1:
                mid = original_count // 2
                print(f"  âš  ì¤„ ìˆ˜ ë¶ˆì¼ì¹˜ ({original_count}ì¤„ ì…ë ¥ -> {len(translated_lines)}ì¤„ ì¶œë ¥, {mid}ì¤„ì”© ë¶„í•  ì¬ì‹œë„)")
                return translate_batch(batch[:mid], file_name) + translate_batch(batch[mid:], file_name)
            else:
                print("  âš  1ì¤„ ë²ˆì—­ ì‹¤íŒ¨: ì›ë³¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
                return batch

        # ì›ë³¸ ë“¤ì—¬ì“°ê¸° íŒ¨í„´ ë³µì›
        result_lines = []
        for i, translated_line in enumerate(translated_lines):
            if i < len(batch):
                # ì›ë³¸ì˜ ë“¤ì—¬ì“°ê¸° ì¶”ì¶œ
                original_line = batch[i]
                indent = original_line[:len(original_line) - len(original_line.lstrip())]
                # ë“¤ì—¬ì“°ê¸°ë¥¼ ë²ˆì—­ëœ ì¤„ì— ì ìš©
                result_lines.append(f"{indent}{translated_line.lstrip()}\n")
            else:
                result_lines.append(translated_line + '\n' if not translated_line.endswith('\n') else translated_line)
        
        return result_lines

    except requests.exceptions.Timeout:
        print(f"  âš  API íƒ€ì„ì•„ì›ƒ (60ì´ˆ ì´ˆê³¼): ì›ë³¸ ìœ ì§€")
        return batch
    except Exception as e:
        print(f"  âš  API ì˜¤ë¥˜: {e}")
        return batch

def has_korean(text):
    """í…ìŠ¤íŠ¸ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    return bool(re.search(r'[ê°€-í£]', text))

def process_file(file_path):
    print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {file_path.name}")
    
    with open(file_path, 'r', encoding='utf-8-sig') as f:
        lines = f.readlines()

    translated_full = []
    batch = []
    max_batch_chars = 300  # 600 â†’ 300ìœ¼ë¡œ ì¶•ì†Œ (ê¸´ ë¬¸ì¥ ì§‘ì¤‘ ë²ˆì—­)
    max_single_line = 2000  # 2000ìê¹Œì§€ í—ˆìš© (ê¸´ ë¬¸ì¥ ë²ˆì—­ ì§€ì›)

    for i, line in enumerate(lines):
        if line.strip().startswith("l_english:"):
            translated_full.append(line.replace("l_english:", "l_korean:"))
            continue
        
        # ë²ˆì—­ ëŒ€ìƒ ì¤„ì¸ì§€ í™•ì¸: ë”°ì˜´í‘œì™€ ì½œë¡ ì´ ìˆê³ , ì£¼ì„ì´ ì•„ë‹ˆë©°, ì´ë¯¸ í•œê¸€ì´ ì—†ëŠ” ê²½ìš°ë§Œ
        if '"' in line and ':' in line and not line.strip().startswith('#') and not has_korean(line):
            current_batch_length = sum(len(l) for l in batch)
            
            # í•œ ì¤„ì´ ë„ˆë¬´ ê¸¸ë©´ ê±´ë„ˆë›°ê¸°
            if len(line) > max_single_line:
                print(f"  âš ï¸ ì¤„ì´ ë„ˆë¬´ ê¹€ (Line {i+1}, {len(line)}ì) - ì›ë³¸ ìœ ì§€")
                translated_full.append(line)
                continue
            
            # í˜„ì¬ ë°°ì¹˜ + ìƒˆ ì¤„ì´ ì œí•œì„ ë„˜ìœ¼ë©´ ë¨¼ì € ë²ˆì—­
            if batch and (current_batch_length + len(line) > max_batch_chars):
                print(f"  ğŸ”„ ë²ˆì—­ ì§„í–‰ ì¤‘... [{i+1}/{len(lines)}]", end='\r')
                translated_full.extend(translate_batch(batch, file_path.name))
                batch = []
                
            batch.append(line)
        else:
            if batch:  # ë²ˆì—­ ëŒ€ìƒì´ ì•„ë‹Œ ì¤„ì„ ë§Œë‚˜ë©´ ì´ì „ ë°°ì¹˜ ì²˜ë¦¬
                translated_full.extend(translate_batch(batch, file_path.name))
                batch = []
            translated_full.append(line)
            
    if batch:
        translated_full.extend(translate_batch(batch, file_path.name))

    with open(file_path, 'w', encoding='utf-8-sig') as f:
        f.writelines(translated_full)
    
    print(f"\nâœ… ì™„ë£Œ: {file_path.name}")

def main():
    current_dir = Path(__file__).parent
    yml_files = [f for f in current_dir.glob("**/*.yml") if "korean" in f.name.lower()]
    
    print(f"ğŸš€ ë²ˆì—­ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: 300ì, í† í°: 8000)")
    for f in yml_files:
        process_file(f)

if __name__ == "__main__":
    main()
